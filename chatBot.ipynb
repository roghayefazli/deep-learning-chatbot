{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roghayefazli/deep-learning-chatbot/blob/main/chatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dPVjYPkCMOL",
        "outputId": "091f2ced-9ade-4bcc-8b7b-783a9d6fc24d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“¥      GitHub...\n",
            "Cloning into 'seq2seq-chatbot'...\n",
            "remote: Enumerating objects: 238, done.\u001b[K\n",
            "remote: Total 238 (delta 0), reused 0 (delta 0), pack-reused 238 (from 1)\u001b[K\n",
            "Receiving objects: 100% (238/238), 15.00 MiB | 10.57 MiB/s, done.\n",
            "Resolving deltas: 100% (112/112), done.\n",
            "/content/data/twitter/seq2seq-chatbot\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nğŸ“¥      GitHub...\")\n",
        "!git clone https://github.com/tensorlayer/seq2seq-chatbot.git\n",
        "%cd seq2seq-chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "w4UiclAiDXFz",
        "outputId": "f96de058-ec78-4522-98f2-ed43c6b84d35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… data downloaded  \n",
            "\n",
            "ğŸ”„ data process   ...\n",
            "âœ… words : 221282\n",
            "\n",
            "ğŸ“š  make words  ...\n",
            "âœ… words size : 8000\n",
            "\n",
            "ğŸ”¢ text to number  ...\n",
            "âœ… train samples  : 199153\n",
            "âœ… test samples  : 22129\n",
            "âœ…   data saved \n",
            "\n",
            "ğŸ¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: embedding=512, hidden=512\n",
            "\n",
            "ğŸ—ï¸  Seq2Seq made \n",
            "\n",
            "ğŸš€ start training ...\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "RNN.__init__() got an unexpected keyword argument 'return_sequences'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-554784950.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nğŸš€ start training ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeq2Seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;31m# Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-554784950.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         self.encoder_rnn = RNN(\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0mcell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRUCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: RNN.__init__() got an unexpected keyword argument 'return_sequences'"
          ]
        }
      ],
      "source": [
        "# Seq2Seq Chatbot -\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 3: Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "from sklearn.utils import shuffle\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¯ÛŒØªØ§Ø³Øª Cornell Movie\n",
        "!mkdir -p data/cornell\n",
        "!wget -q -O cornell_movie_dialogs.zip http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
        "!unzip -q -o cornell_movie_dialogs.zip -d data/cornell/\n",
        "!mv data/cornell/cornell\\ movie-dialogs\\ corpus/* data/cornell/\n",
        "!rm cornell_movie_dialogs.zip\n",
        "\n",
        "print(\"âœ… data downloaded  \")\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 4: Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
        "# ============================================\n",
        "print(\"\\nğŸ”„ data process   ...\")\n",
        "\n",
        "def load_cornell_data():\n",
        "    \"\"\"Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯ÛŒØªØ§Ø³Øª Cornell Movie\"\"\"\n",
        "    # Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§\n",
        "    lines_file = 'data/cornell/movie_lines.txt'\n",
        "    conv_file = 'data/cornell/movie_conversations.txt'\n",
        "\n",
        "    # Ø®ÙˆØ§Ù†Ø¯Ù† ØªÙ…Ø§Ù… Ø®Ø·ÙˆØ·\n",
        "    id2line = {}\n",
        "    with open(lines_file, 'r', encoding='iso-8859-1') as f:\n",
        "        for line in f:\n",
        "            parts = line.split(' +++$+++ ')\n",
        "            if len(parts) == 5:\n",
        "                id2line[parts[0]] = parts[4].strip()\n",
        "\n",
        "    # Ø®ÙˆØ§Ù†Ø¯Ù† Ù…Ú©Ø§Ù„Ù…Ø§Øª\n",
        "    convs = []\n",
        "    with open(conv_file, 'r', encoding='iso-8859-1') as f:\n",
        "        for line in f:\n",
        "            parts = line.split(' +++$+++ ')\n",
        "            if len(parts) == 4:\n",
        "                conv = eval(parts[3])\n",
        "                convs.append(conv)\n",
        "\n",
        "    # Ø³Ø§Ø®Øª Ø¬ÙØªâ€ŒÙ‡Ø§ÛŒ Ø³ÙˆØ§Ù„-Ø¬ÙˆØ§Ø¨\n",
        "    questions = []\n",
        "    answers = []\n",
        "\n",
        "    for conv in convs:\n",
        "        for i in range(len(conv) - 1):\n",
        "            q = id2line.get(conv[i], '')\n",
        "            a = id2line.get(conv[i+1], '')\n",
        "            if q and a:\n",
        "                questions.append(q)\n",
        "                answers.append(a)\n",
        "\n",
        "    return questions, answers\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ù…ØªÙ†\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}+=~|.?,]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
        "questions, answers = load_cornell_data()\n",
        "print(f\"âœ… words : {len(questions)}\")\n",
        "\n",
        "# ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù†\n",
        "questions = [clean_text(q) for q in questions]\n",
        "answers = [clean_text(a) for a in answers]\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 5: Ø³Ø§Ø®Øª ÙˆØ§Ú˜Ú¯Ø§Ù†\n",
        "# ============================================\n",
        "print(\"\\nğŸ“š  make words  ...\")\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Ø´Ù…Ø§Ø±Ø´ Ú©Ù„Ù…Ø§Øª\n",
        "word_counts = Counter()\n",
        "for question in questions:\n",
        "    word_counts.update(question.split())\n",
        "for answer in answers:\n",
        "    word_counts.update(answer.split())\n",
        "\n",
        "# Ø§Ù†ØªØ®Ø§Ø¨ top Ú©Ù„Ù…Ø§Øª\n",
        "vocab_size = 8000\n",
        "most_common = word_counts.most_common(vocab_size - 4)  # -4 Ø¨Ø±Ø§ÛŒ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ\n",
        "\n",
        "# Ø³Ø§Ø®Øª ÙˆØ§Ú˜Ú¯Ø§Ù†\n",
        "w2idx = {'<pad>': 0, '<unk>': 1, '<start>': 2, '<end>': 3}\n",
        "idx2w = {0: '<pad>', 1: '<unk>', 2: '<start>', 3: '<end>'}\n",
        "\n",
        "for i, (word, _) in enumerate(most_common, start=4):\n",
        "    w2idx[word] = i\n",
        "    idx2w[i] = word\n",
        "\n",
        "vocab_size = len(w2idx)\n",
        "print(f\"âœ… words size : {vocab_size}\")\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 6: ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø§Ø¹Ø¯Ø§Ø¯\n",
        "# ============================================\n",
        "print(\"\\nğŸ”¢ text to number  ...\")\n",
        "\n",
        "def text_to_ids(texts, w2idx, max_len=20):\n",
        "    \"\"\"ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ù„ÛŒØ³Øª Ø§Ø¹Ø¯Ø§Ø¯\"\"\"\n",
        "    sequences = []\n",
        "    for text in texts:\n",
        "        words = text.split()[:max_len]\n",
        "        ids = [w2idx.get(w, w2idx['<unk>']) for w in words]\n",
        "        sequences.append(ids)\n",
        "    return sequences\n",
        "\n",
        "trainX = text_to_ids(questions, w2idx)\n",
        "trainY = text_to_ids(answers, w2idx)\n",
        "\n",
        "# ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ train/test\n",
        "split_idx = int(len(trainX) * 0.9)\n",
        "testX = trainX[split_idx:]\n",
        "testY = trainY[split_idx:]\n",
        "trainX = trainX[:split_idx]\n",
        "trainY = trainY[:split_idx]\n",
        "\n",
        "# ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ numpy array\n",
        "trainX = np.array(trainX, dtype=object)\n",
        "trainY = np.array(trainY, dtype=object)\n",
        "testX = np.array(testX, dtype=object)\n",
        "testY = np.array(testY, dtype=object)\n",
        "\n",
        "print(f\"âœ… train samples  : {len(trainX)}\")\n",
        "print(f\"âœ… test samples  : {len(testX)}\")\n",
        "\n",
        "# Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ø¹Ø¯ÛŒ\n",
        "os.makedirs('data/processed', exist_ok=True)\n",
        "metadata = {'w2idx': w2idx, 'idx2w': idx2w}\n",
        "with open('data/processed/metadata.pkl', 'wb') as f:\n",
        "    pickle.dump(metadata, f)\n",
        "np.save('data/processed/trainX.npy', trainX)\n",
        "np.save('data/processed/trainY.npy', trainY)\n",
        "np.save('data/processed/testX.npy', testX)\n",
        "np.save('data/processed/testY.npy', testY)\n",
        "\n",
        "print(\"âœ…   data saved \")\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 7: ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„\n",
        "# ============================================\n",
        "embedding_size = 512\n",
        "hidden_size = 512\n",
        "num_layers = 2\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20  # Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹ - Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯\n",
        "\n",
        "model_dir = 'model'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "print(f\"\\nğŸ¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: embedding={embedding_size}, hidden={hidden_size}\")\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 8: ØªØ¹Ø±ÛŒÙ Ù…Ø¯Ù„ Seq2Seq\n",
        "# ============================================\n",
        "from tensorlayer.layers import *\n",
        "from tensorlayer.models import Model\n",
        "\n",
        "class Seq2Seq(Model):\n",
        "    def __init__(self):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder_embedding = Embedding(\n",
        "            vocabulary_size=vocab_size,\n",
        "            embedding_size=embedding_size,\n",
        "            name='encoder_embedding'\n",
        "        )\n",
        "\n",
        "        self.encoder_rnn = RNN(\n",
        "            cell=tf.keras.layers.GRUCell(hidden_size, dropout=0.1),\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            name='encoder_rnn'\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_embedding = Embedding(\n",
        "            vocabulary_size=vocab_size,\n",
        "            embedding_size=embedding_size,\n",
        "            name='decoder_embedding'\n",
        "        )\n",
        "\n",
        "        self.decoder_rnn = RNN(\n",
        "            cell=tf.keras.layers.GRUCell(hidden_size, dropout=0.1),\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            name='decoder_rnn'\n",
        "        )\n",
        "\n",
        "        self.decoder_dense = Dense(\n",
        "            n_units=vocab_size,\n",
        "            name='decoder_dense'\n",
        "        )\n",
        "\n",
        "    def forward(self, encoder_inputs, decoder_inputs, initial_state=None):\n",
        "        # Encoder\n",
        "        encoder_embeddings = self.encoder_embedding(encoder_inputs)\n",
        "        encoder_outputs, encoder_state = self.encoder_rnn(encoder_embeddings, initial_state)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_embeddings = self.decoder_embedding(decoder_inputs)\n",
        "        decoder_outputs, decoder_state = self.decoder_rnn(decoder_embeddings, encoder_state)\n",
        "        decoder_logits = self.decoder_dense(decoder_outputs)\n",
        "\n",
        "        return decoder_logits, decoder_state\n",
        "\n",
        "print(\"\\nğŸ—ï¸  Seq2Seq made \")\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 9: ØªØ§Ø¨Ø¹ Loss\n",
        "# ============================================\n",
        "def cross_entropy_seq_with_mask(logits, target_seqs, input_mask):\n",
        "    \"\"\"Ù…Ø­Ø§Ø³Ø¨Ù‡ cross-entropy Ø¨Ø§ mask Ø¨Ø±Ø§ÛŒ padding\"\"\"\n",
        "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        logits=logits,\n",
        "        labels=target_seqs\n",
        "    )\n",
        "    losses = losses * input_mask\n",
        "    loss = tf.reduce_sum(losses) / (tf.reduce_sum(input_mask) + 1e-8)\n",
        "    return loss\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 10: Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„\n",
        "# ============================================\n",
        "def train_model(model, trainX, trainY, num_epochs=20):\n",
        "    \"\"\"ØªØ§Ø¨Ø¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„\"\"\"\n",
        "    optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    n_step = len(trainX) // batch_size\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        trainX_shuffled, trainY_shuffled = shuffle(trainX, trainY)\n",
        "        total_loss = 0\n",
        "\n",
        "        pbar = tqdm(\n",
        "            tl.iterate.minibatches(trainX_shuffled, trainY_shuffled, batch_size, shuffle=False),\n",
        "            total=n_step,\n",
        "            desc=f'Epoch [{epoch+1}/{num_epochs}]',\n",
        "            leave=True\n",
        "        )\n",
        "\n",
        "        step = 0\n",
        "        for X, Y in pbar:\n",
        "            # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
        "            X = tl.prepro.pad_sequences(X)\n",
        "            _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=w2idx['<end>'])\n",
        "            _target_seqs = tl.prepro.pad_sequences(_target_seqs)\n",
        "            _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=w2idx['<start>'], remove_last=False)\n",
        "            _decode_seqs = tl.prepro.pad_sequences(_decode_seqs)\n",
        "            _target_mask = tl.prepro.sequences_get_mask(_target_seqs)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                output, _ = model(X, _decode_seqs)\n",
        "                loss = cross_entropy_seq_with_mask(output, _target_seqs, _target_mask)\n",
        "\n",
        "            gradients = tape.gradient(loss, model.trainable_weights)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "            total_loss += loss\n",
        "            step += 1\n",
        "            pbar.set_postfix(loss=f'{loss:.4f}')\n",
        "\n",
        "        avg_loss = total_loss / step\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            model_path = os.path.join(model_dir, f'model_epoch_{epoch+1}.npz')\n",
        "            tl.files.save_npz(model.trainable_weights, model_path)\n",
        "            print(f\"ğŸ’¾ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {model_path}\")\n",
        "\n",
        "print(\"\\nğŸš€ start training ...\")\n",
        "model = Seq2Seq()\n",
        "\n",
        "# Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡\n",
        "sample_X = np.zeros((1, 10), dtype=np.int32)\n",
        "sample_Y = np.zeros((1, 10), dtype=np.int32)\n",
        "_ = model(sample_X, sample_Y)\n",
        "\n",
        "print(\"âœ…   ready !\")\n",
        "print(f\"ğŸ“Š  parameters : {len(model.trainable_weights)}\")\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 11: Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´\n",
        "# ============================================\n",
        "train_model(model, trainX, trainY, num_epochs=num_epochs)\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 12: ØªØ§Ø¨Ø¹ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® (Inference)\n",
        "# ============================================\n",
        "def inference(seed, model, top_n=3):\n",
        "    \"\"\"ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ø¬Ù…Ù„Ù‡ ÙˆØ±ÙˆØ¯ÛŒ\"\"\"\n",
        "    seed = clean_text(seed)\n",
        "    seed_id = [w2idx.get(w, w2idx['<unk>']) for w in seed.split()]\n",
        "\n",
        "    if not seed_id:\n",
        "        return \"sorry i dont understand\"\n",
        "\n",
        "    # Encode\n",
        "    encoder_input = tl.prepro.pad_sequences([seed_id])\n",
        "    encoder_embeddings = model.encoder_embedding(encoder_input)\n",
        "    _, state = model.encoder_rnn(encoder_embeddings)\n",
        "\n",
        "    # Decode\n",
        "    decoder_input = np.array([[w2idx['<start>']]])\n",
        "    sentence = []\n",
        "\n",
        "    for _ in range(20):  # Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ Ù¾Ø§Ø³Ø®\n",
        "        decoder_embeddings = model.decoder_embedding(decoder_input)\n",
        "        decoder_outputs, state = model.decoder_rnn(decoder_embeddings, state)\n",
        "        logits = model.decoder_dense(decoder_outputs)\n",
        "\n",
        "        # Ø§Ù†ØªØ®Ø§Ø¨ top_n Ú©Ù„Ù…Ù‡\n",
        "        probs = tf.nn.softmax(logits[0, -1]).numpy()\n",
        "        top_indices = np.argsort(probs)[-top_n:]\n",
        "\n",
        "        # Ø§Ù†ØªØ®Ø§Ø¨ ØªØµØ§Ø¯ÙÛŒ Ø§Ø² top_n\n",
        "        word_id = np.random.choice(top_indices)\n",
        "\n",
        "        if word_id == w2idx['<end>'] or word_id == w2idx['<pad>']:\n",
        "            break\n",
        "\n",
        "        word = idx2w[word_id]\n",
        "        sentence.append(word)\n",
        "        decoder_input = np.array([[word_id]])\n",
        "\n",
        "    return ' '.join(sentence) if sentence else \"...\"\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 13: ØªØ³Øª ØªØ¹Ø§Ù…Ù„ÛŒ\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ğŸ¤– chatbot is ready !\")\n",
        "print(\"=\"*50)\n",
        "print(\" questions :\\n\")\n",
        "\n",
        "# ØªØ³Øª Ú†Ù†Ø¯ Ù†Ù…ÙˆÙ†Ù‡\n",
        "test_queries = [\n",
        "    \"hello\",\n",
        "    \"how are you\",\n",
        "    \"what is your name\",\n",
        "    \"thank you\",\n",
        "    \"good morning\",\n",
        "    \"bye\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    response = inference(query, model, top_n=3)\n",
        "    print(f\"You: {query}\")\n",
        "    print(f\"Bot: {response}\\n\")\n",
        "\n",
        "# Ø­Ù„Ù‚Ù‡ ØªØ¹Ø§Ù…Ù„ÛŒ\n",
        "print(\"\\nğŸ’¬ ask question  (  'quit' ):\\n\")\n",
        "while True:\n",
        "    try:\n",
        "        query = input(\"You: \")\n",
        "        if query.lower() in ['quit', 'exit', 'q', '']:\n",
        "            print(\"Ø®Ø¯Ø§Ø­Ø§ÙØ¸! ğŸ‘‹\")\n",
        "            break\n",
        "\n",
        "        if query.strip():\n",
        "            response = inference(query, model, top_n=3)\n",
        "            print(f\"Bot: {response}\\n\")\n",
        "    except:\n",
        "        break\n",
        "\n",
        "print(\"\\nâœ… Ø§Ø¬Ø±Ø§ ØªÙ…Ø§Ù… Ø´Ø¯!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjsZU2N6IgMx",
        "outputId": "3b61a0fd-a943-4df7-aabc-ef2fa1bc8d16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“¥ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ø¯ Ø§Ø² GitHub...\n",
            "Cloning into 'seq2seq-chatbot'...\n",
            "remote: Enumerating objects: 238, done.\u001b[K\n",
            "remote: Total 238 (delta 0), reused 0 (delta 0), pack-reused 238 (from 1)\u001b[K\n",
            "Receiving objects: 100% (238/238), 15.00 MiB | 5.53 MiB/s, done.\n",
            "Resolving deltas: 100% (112/112), done.\n",
            "/content/data/twitter/seq2seq-chatbot/seq2seq-chatbot/seq2seq-chatbot/seq2seq-chatbot/seq2seq-chatbot/seq2seq-chatbot/seq2seq-chatbot/seq2seq-chatbot/seq2seq-chatbot\n",
            "\n",
            "ğŸ“¥ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...\n",
            "âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù†Ø¯\n",
            "\n",
            "ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...\n",
            "âœ… ØªØ¹Ø¯Ø§Ø¯ Ù…Ú©Ø§Ù„Ù…Ø§Øª: 221282\n",
            "\n",
            "ğŸ“š Ø¯Ø± Ø­Ø§Ù„ Ø³Ø§Ø®Øª ÙˆØ§Ú˜Ú¯Ø§Ù†...\n",
            "âœ… Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù†: 8000\n",
            "\n",
            "ğŸ”¢ Ø¯Ø± Ø­Ø§Ù„ ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø§Ø¹Ø¯Ø§Ø¯...\n",
            "âœ… ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´: 199153\n",
            "âœ… ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª: 22129\n",
            "âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯\n",
            "\n",
            "ğŸ¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: embedding=512, hidden=512\n",
            "\n",
            "ğŸ—ï¸ Ù…Ø¯Ù„ Seq2Seq Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 2: Ú©Ù„ÙˆÙ† Ú©Ø±Ø¯Ù† Ø±ÛŒÙ¾Ø§Ø²ÛŒØªÙˆØ±ÛŒ\n",
        "# ============================================\n",
        "print(\"\\nğŸ“¥ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ø¯ Ø§Ø² GitHub...\")\n",
        "!rm -rf seq2seq-chatbot\n",
        "!git clone https://github.com/tensorlayer/seq2seq-chatbot.git\n",
        "%cd seq2seq-chatbot\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 3: Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
        "# ============================================\n",
        "print(\"\\nğŸ“¥ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...\")\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "from sklearn.utils import shuffle\n",
        "from tqdm import tqdm\n",
        "from tensorlayer.layers import *\n",
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "from tensorlayer.layers import Embedding, Dense\n",
        "\n",
        "# Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¯ÛŒØªØ§Ø³Øª Cornell Movie\n",
        "!mkdir -p data/cornell\n",
        "!wget -q -O cornell_movie_dialogs.zip http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
        "!unzip -q -o cornell_movie_dialogs.zip -d data/cornell/\n",
        "!mv data/cornell/cornell\\ movie-dialogs\\ corpus/* data/cornell/\n",
        "!rm cornell_movie_dialogs.zip\n",
        "\n",
        "print(\"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù†Ø¯\")\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 4: Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
        "# ============================================\n",
        "print(\"\\nğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...\")\n",
        "\n",
        "def load_cornell_data():\n",
        "    \"\"\"Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯ÛŒØªØ§Ø³Øª Cornell Movie\"\"\"\n",
        "    # Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§\n",
        "    lines_file = 'data/cornell/movie_lines.txt'\n",
        "    conv_file = 'data/cornell/movie_conversations.txt'\n",
        "\n",
        "    # Ø®ÙˆØ§Ù†Ø¯Ù† ØªÙ…Ø§Ù… Ø®Ø·ÙˆØ·\n",
        "    id2line = {}\n",
        "    with open(lines_file, 'r', encoding='iso-8859-1') as f:\n",
        "        for line in f:\n",
        "            parts = line.split(' +++$+++ ')\n",
        "            if len(parts) == 5:\n",
        "                id2line[parts[0]] = parts[4].strip()\n",
        "\n",
        "    # Ø®ÙˆØ§Ù†Ø¯Ù† Ù…Ú©Ø§Ù„Ù…Ø§Øª\n",
        "    convs = []\n",
        "    with open(conv_file, 'r', encoding='iso-8859-1') as f:\n",
        "        for line in f:\n",
        "            parts = line.split(' +++$+++ ')\n",
        "            if len(parts) == 4:\n",
        "                conv = eval(parts[3])\n",
        "                convs.append(conv)\n",
        "\n",
        "    # Ø³Ø§Ø®Øª Ø¬ÙØªâ€ŒÙ‡Ø§ÛŒ Ø³ÙˆØ§Ù„-Ø¬ÙˆØ§Ø¨\n",
        "    questions = []\n",
        "    answers = []\n",
        "\n",
        "    for conv in convs:\n",
        "        for i in range(len(conv) - 1):\n",
        "            q = id2line.get(conv[i], '')\n",
        "            a = id2line.get(conv[i+1], '')\n",
        "            if q and a:\n",
        "                questions.append(q)\n",
        "                answers.append(a)\n",
        "\n",
        "    return questions, answers\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ù…ØªÙ†\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}+=~|.?,]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
        "questions, answers = load_cornell_data()\n",
        "print(f\"âœ… ØªØ¹Ø¯Ø§Ø¯ Ù…Ú©Ø§Ù„Ù…Ø§Øª: {len(questions)}\")\n",
        "\n",
        "# ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù†\n",
        "questions = [clean_text(q) for q in questions]\n",
        "answers = [clean_text(a) for a in answers]\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 5: Ø³Ø§Ø®Øª ÙˆØ§Ú˜Ú¯Ø§Ù†\n",
        "# ============================================\n",
        "print(\"\\nğŸ“š Ø¯Ø± Ø­Ø§Ù„ Ø³Ø§Ø®Øª ÙˆØ§Ú˜Ú¯Ø§Ù†...\")\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Ø´Ù…Ø§Ø±Ø´ Ú©Ù„Ù…Ø§Øª\n",
        "word_counts = Counter()\n",
        "for question in questions:\n",
        "    word_counts.update(question.split())\n",
        "for answer in answers:\n",
        "    word_counts.update(answer.split())\n",
        "\n",
        "# Ø§Ù†ØªØ®Ø§Ø¨ top Ú©Ù„Ù…Ø§Øª\n",
        "vocab_size = 8000\n",
        "most_common = word_counts.most_common(vocab_size - 4)  # -4 Ø¨Ø±Ø§ÛŒ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ\n",
        "\n",
        "# Ø³Ø§Ø®Øª ÙˆØ§Ú˜Ú¯Ø§Ù†\n",
        "w2idx = {'<pad>': 0, '<unk>': 1, '<start>': 2, '<end>': 3}\n",
        "idx2w = {0: '<pad>', 1: '<unk>', 2: '<start>', 3: '<end>'}\n",
        "\n",
        "for i, (word, _) in enumerate(most_common, start=4):\n",
        "    w2idx[word] = i\n",
        "    idx2w[i] = word\n",
        "\n",
        "vocab_size = len(w2idx)\n",
        "print(f\"âœ… Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù†: {vocab_size}\")\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 6: ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø§Ø¹Ø¯Ø§Ø¯\n",
        "# ============================================\n",
        "print(\"\\nğŸ”¢ Ø¯Ø± Ø­Ø§Ù„ ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø§Ø¹Ø¯Ø§Ø¯...\")\n",
        "\n",
        "def text_to_ids(texts, w2idx, max_len=20):\n",
        "    \"\"\"ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ù„ÛŒØ³Øª Ø§Ø¹Ø¯Ø§Ø¯\"\"\"\n",
        "    sequences = []\n",
        "    for text in texts:\n",
        "        words = text.split()[:max_len]\n",
        "        ids = [w2idx.get(w, w2idx['<unk>']) for w in words]\n",
        "        sequences.append(ids)\n",
        "    return sequences\n",
        "\n",
        "trainX = text_to_ids(questions, w2idx)\n",
        "trainY = text_to_ids(answers, w2idx)\n",
        "\n",
        "# ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ train/test\n",
        "split_idx = int(len(trainX) * 0.9)\n",
        "testX = trainX[split_idx:]\n",
        "testY = trainY[split_idx:]\n",
        "trainX = trainX[:split_idx]\n",
        "trainY = trainY[:split_idx]\n",
        "\n",
        "# ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ numpy array\n",
        "trainX = np.array(trainX, dtype=object)\n",
        "trainY = np.array(trainY, dtype=object)\n",
        "testX = np.array(testX, dtype=object)\n",
        "testY = np.array(testY, dtype=object)\n",
        "\n",
        "print(f\"âœ… ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´: {len(trainX)}\")\n",
        "print(f\"âœ… ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª: {len(testX)}\")\n",
        "\n",
        "# Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ø¹Ø¯ÛŒ\n",
        "os.makedirs('data/processed', exist_ok=True)\n",
        "metadata = {'w2idx': w2idx, 'idx2w': idx2w}\n",
        "with open('data/processed/metadata.pkl', 'wb') as f:\n",
        "    pickle.dump(metadata, f)\n",
        "np.save('data/processed/trainX.npy', trainX)\n",
        "np.save('data/processed/trainY.npy', trainY)\n",
        "np.save('data/processed/testX.npy', testX)\n",
        "np.save('data/processed/testY.npy', testY)\n",
        "\n",
        "print(\"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯\")\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 7: ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„\n",
        "# ============================================\n",
        "embedding_size = 512\n",
        "hidden_size = 512\n",
        "num_layers = 2\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20  # Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹ - Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯\n",
        "\n",
        "model_dir = 'model'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "print(f\"\\nğŸ¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: embedding={embedding_size}, hidden={hidden_size}\")\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 8: ØªØ¹Ø±ÛŒÙ Ù…Ø¯Ù„ Seq2Seq\n",
        "# ============================================\n",
        "from tensorlayer.layers import *\n",
        "from tensorlayer.models import Model\n",
        "\n",
        "class Seq2Seq(tl.models.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Embeddings (TensorLayer - Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ in_channels)\n",
        "        self.encoder_embedding = tl.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.decoder_embedding = tl.layers.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "        # Keras LSTM layers (Ø¨Ù‡ØªØ±ÛŒÙ† Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ TL 2 + TF 2)\n",
        "        self.encoder_lstm = tf.keras.layers.LSTM(\n",
        "            hidden_size, return_sequences=True, return_state=True\n",
        "        )\n",
        "        self.decoder_lstm = tf.keras.layers.LSTM(\n",
        "            hidden_size, return_sequences=True, return_state=True\n",
        "        )\n",
        "\n",
        "        # Keras Dense layer (Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ in_channels)\n",
        "        self.decoder_dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def forward(self, encoder_inputs, decoder_inputs):\n",
        "        # Encoder\n",
        "        enc_emb = self.encoder_embedding(encoder_inputs)\n",
        "        enc_output, enc_h, enc_c = self.encoder_lstm(enc_emb)\n",
        "\n",
        "        # Decoder\n",
        "        dec_emb = self.decoder_embedding(decoder_inputs)\n",
        "        dec_output, _, _ = self.decoder_lstm(dec_emb, initial_state=[enc_h, enc_c])\n",
        "        logits = self.decoder_dense(dec_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "print(\"\\nğŸ—ï¸ Ù…Ø¯Ù„ Seq2Seq Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯\")\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 9: ØªØ§Ø¨Ø¹ Loss\n",
        "# ============================================\n",
        "def cross_entropy_seq_with_mask(logits, target_seqs, input_mask):\n",
        "    \"\"\"Ù…Ø­Ø§Ø³Ø¨Ù‡ cross-entropy Ø¨Ø§ mask Ø¨Ø±Ø§ÛŒ padding\"\"\"\n",
        "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        logits=logits,\n",
        "        labels=target_seqs\n",
        "    )\n",
        "    losses = losses * input_mask\n",
        "    loss = tf.reduce_sum(losses) / (tf.reduce_sum(input_mask) + 1e-8)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HW62Cf-qImId"
      },
      "outputs": [],
      "source": [
        "def train_model(model, trainX, trainY, num_epochs=20):\n",
        "    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù„ÛŒØ³Øª Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø´Ú©Ù„Ø§Øª dtype=object\n",
        "    trainX = [list(seq) for seq in trainX]\n",
        "    trainY = [list(seq) for seq in trainY]\n",
        "\n",
        "    optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "    n_step = len(trainX) // batch_size\n",
        "\n",
        "    # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† batch Ù†Ø§Ù‚Øµ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ ÙˆÙ„ÛŒ ØªÙˆØµÛŒÙ‡â€ŒØ´Ø¯Ù‡)\n",
        "    num_examples = n_step * batch_size\n",
        "    trainX, trainY = trainX[:num_examples], trainY[:num_examples]\n",
        "    n_step = len(trainX) // batch_size\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        trainX_shuffled, trainY_shuffled = shuffle(trainX, trainY)\n",
        "        total_loss = 0\n",
        "\n",
        "        pbar = tqdm(\n",
        "            tl.iterate.minibatches(trainX_shuffled, trainY_shuffled, batch_size, shuffle=False),\n",
        "            total=n_step,\n",
        "            desc=f'Epoch [{epoch+1}/{num_epochs}]',\n",
        "            leave=True\n",
        "        )\n",
        "\n",
        "        step = 0\n",
        "        for X, Y in pbar:\n",
        "            if len(X) != batch_size:\n",
        "                continue  # Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ† batch Ù†Ø§Ù‚Øµ\n",
        "\n",
        "            X = tl.prepro.pad_sequences(X)\n",
        "            _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=w2idx['<end>'])\n",
        "            _target_seqs = tl.prepro.pad_sequences(_target_seqs)\n",
        "            _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=w2idx['<start>'], remove_last=False)\n",
        "            _decode_seqs = tl.prepro.pad_sequences(_decode_seqs)\n",
        "            _target_mask = tl.prepro.sequences_get_mask(_target_seqs)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                output = model(X, _decode_seqs)  # âœ… Ù‡Ø± Ø¯Ùˆ ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡\n",
        "                loss = cross_entropy_seq_with_mask(output, _target_seqs, _target_mask)\n",
        "\n",
        "            gradients = tape.gradient(loss, model.trainable_weights)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "            total_loss += loss\n",
        "            step += 1\n",
        "            pbar.set_postfix(loss=f'{loss:.4f}')\n",
        "\n",
        "        if step > 0:\n",
        "            avg_loss = total_loss / step\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] - Average Loss: {avg_loss:.4f}\")\n",
        "        else:\n",
        "            print(\"No valid batches in this epoch!\")\n",
        "            continue\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            model_path = os.path.join(model_dir, f'model_epoch_{epoch+1}.npz')\n",
        "            tl.files.save_npz(model.trainable_weights, model_path)\n",
        "            print(f\"ğŸ’¾ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "kOlKJ4vGIpRf",
        "outputId": "22e4cb4a-e70e-4db0-88ec-69665a2e1c64"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'shape'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1674016055.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m_decode_seqs_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decode_seqs_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test input shapes:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_decode_seqs_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0moutput_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_decode_seqs_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test output shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ø¨Ø§ÛŒØ¯ (batch, seq_len, vocab_size) Ø¨Ø§Ø´Ø¯\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "# ØªØ³Øª ÛŒÚ© batch\n",
        "X_test, Y_test = next(tl.iterate.minibatches(trainX, trainY, batch_size, shuffle=False))\n",
        "X_test = tl.prepro.pad_sequences(X_test)\n",
        "_decode_seqs_test = tl.prepro.sequences_add_start_id(Y_test, start_id=w2idx['<start>'], remove_last=False)\n",
        "_decode_seqs_test = tl.prepro.pad_sequences(_decode_seqs_test)\n",
        "\n",
        "print(\"Test input shapes:\", X_test.shape, _decode_seqs_test.shape)\n",
        "output_test = model(X_test, _decode_seqs_test)\n",
        "print(\"Test output shape:\", output_test.shape)  # Ø¨Ø§ÛŒØ¯ (batch, seq_len, vocab_size) Ø¨Ø§Ø´Ø¯"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "vcEiorLhExuX",
        "outputId": "0fa807f6-c68c-442c-adcf-4e321bbee1de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“¥ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ø¯ Ø§Ø² GitHub...\n",
            "Cloning into 'seq2seq-chatbot'...\n",
            "remote: Enumerating objects: 238, done.\u001b[K\n",
            "remote: Total 238 (delta 0), reused 0 (delta 0), pack-reused 238 (from 1)\u001b[K\n",
            "Receiving objects: 100% (238/238), 15.00 MiB | 12.81 MiB/s, done.\n",
            "Resolving deltas: 100% (112/112), done.\n",
            "/content/data/twitter/seq2seq-chatbot/seq2seq-chatbot/seq2seq-chatbot/seq2seq-chatbot/seq2seq-chatbot/seq2seq-chatbot/seq2seq-chatbot/seq2seq-chatbot\n",
            "\n",
            "ğŸ“¥ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...\n",
            "âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù†Ø¯\n",
            "\n",
            "ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...\n",
            "âœ… ØªØ¹Ø¯Ø§Ø¯ Ù…Ú©Ø§Ù„Ù…Ø§Øª: 221282\n",
            "\n",
            "ğŸ“š Ø¯Ø± Ø­Ø§Ù„ Ø³Ø§Ø®Øª ÙˆØ§Ú˜Ú¯Ø§Ù†...\n",
            "âœ… Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù†: 8000\n",
            "\n",
            "ğŸ”¢ Ø¯Ø± Ø­Ø§Ù„ ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø§Ø¹Ø¯Ø§Ø¯...\n",
            "âœ… ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´: 199153\n",
            "âœ… ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª: 22129\n",
            "âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯\n",
            "\n",
            "ğŸ¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: embedding=512, hidden=512\n",
            "\n",
            "ğŸ—ï¸ Ù…Ø¯Ù„ Seq2Seq Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯\n",
            "\n",
            "ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„...\n",
            "\n",
            "ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„...\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Seq2Seq.forward() missing 1 required positional argument: 'decoder_inputs'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2681888657.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0msample_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0msample_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"âœ… Ù…Ø¯Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorlayer/models/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, is_train, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Seq2Seq.forward() missing 1 required positional argument: 'decoder_inputs'"
          ]
        }
      ],
      "source": [
        "# Seq2Seq Chatbot - Google Colab Version (Ø¨Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡)\n",
        "# Ù†Ø³Ø®Ù‡ ØªØ·Ø¨ÛŒÙ‚ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ Ø¯Ø± Google Colab\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 1: Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²\n",
        "# ============================================\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 10: Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„\n",
        "# ============================================\n",
        "def train_model(model, trainX, trainY, num_epochs=20):\n",
        "    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù„ÛŒØ³Øª Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø´Ú©Ù„Ø§Øª dtype=object\n",
        "    trainX = [list(seq) for seq in trainX]\n",
        "    trainY = [list(seq) for seq in trainY]\n",
        "\n",
        "    optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "    n_step = len(trainX) // batch_size\n",
        "\n",
        "    # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† batch Ù†Ø§Ù‚Øµ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ ÙˆÙ„ÛŒ ØªÙˆØµÛŒÙ‡â€ŒØ´Ø¯Ù‡)\n",
        "    num_examples = n_step * batch_size\n",
        "    trainX, trainY = trainX[:num_examples], trainY[:num_examples]\n",
        "    n_step = len(trainX) // batch_size\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        trainX_shuffled, trainY_shuffled = shuffle(trainX, trainY)\n",
        "        total_loss = 0\n",
        "\n",
        "        pbar = tqdm(\n",
        "            tl.iterate.minibatches(trainX_shuffled, trainY_shuffled, batch_size, shuffle=False),\n",
        "            total=n_step,\n",
        "            desc=f'Epoch [{epoch+1}/{num_epochs}]',\n",
        "            leave=True\n",
        "        )\n",
        "\n",
        "        step = 0\n",
        "        for X, Y in pbar:\n",
        "            if len(X) != batch_size:\n",
        "                continue  # Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ† batch Ù†Ø§Ù‚Øµ\n",
        "\n",
        "            X = tl.prepro.pad_sequences(X)\n",
        "            _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=w2idx['<end>'])\n",
        "            _target_seqs = tl.prepro.pad_sequences(_target_seqs)\n",
        "            _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=w2idx['<start>'], remove_last=False)\n",
        "            _decode_seqs = tl.prepro.pad_sequences(_decode_seqs)\n",
        "            _target_mask = tl.prepro.sequences_get_mask(_target_seqs)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                output = model(X, _decode_seqs)  # âœ… Ù‡Ø± Ø¯Ùˆ ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡\n",
        "                loss = cross_entropy_seq_with_mask(output, _target_seqs, _target_mask)\n",
        "\n",
        "            gradients = tape.gradient(loss, model.trainable_weights)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "            total_loss += loss\n",
        "            step += 1\n",
        "            pbar.set_postfix(loss=f'{loss:.4f}')\n",
        "\n",
        "        if step > 0:\n",
        "            avg_loss = total_loss / step\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] - Average Loss: {avg_loss:.4f}\")\n",
        "        else:\n",
        "            print(\"No valid batches in this epoch!\")\n",
        "            continue\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            model_path = os.path.join(model_dir, f'model_epoch_{epoch+1}.npz')\n",
        "            tl.files.save_npz(model.trainable_weights, model_path)\n",
        "            print(f\"ğŸ’¾ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {model_path}\")\n",
        "\n",
        "print(\"\\nğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„...\")\n",
        "print(\"\\nğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„...\")\n",
        "model = Seq2Seq(vocab_size=vocab_size, embedding_size=embedding_size, hidden_size=hidden_size)\n",
        "# Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡\n",
        "sample_X = np.zeros((1, 10), dtype=np.int32)\n",
        "sample_Y = np.zeros((1, 10), dtype=np.int32)\n",
        "_ = model(sample_X, sample_Y)\n",
        "\n",
        "print(\"âœ… Ù…Ø¯Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª!\")\n",
        "print(f\"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: {len(model.trainable_weights)}\")\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 11: Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´\n",
        "# ============================================\n",
        "train_model(model, trainX, trainY, num_epochs=num_epochs)\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 12: ØªØ§Ø¨Ø¹ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® (Inference)\n",
        "# ============================================\n",
        "def inference(seed, model, top_n=3):\n",
        "    \"\"\"ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ø¬Ù…Ù„Ù‡ ÙˆØ±ÙˆØ¯ÛŒ\"\"\"\n",
        "    seed = clean_text(seed)\n",
        "    seed_id = [w2idx.get(w, w2idx['<unk>']) for w in seed.split()]\n",
        "\n",
        "    if not seed_id:\n",
        "        return \"sorry i dont understand\"\n",
        "\n",
        "    # Encode\n",
        "    encoder_input = tl.prepro.pad_sequences([seed_id])\n",
        "    encoder_embeddings = model.encoder_embedding(encoder_input)\n",
        "    _, encoder_states = model.encoder_lstm(encoder_embeddings)\n",
        "\n",
        "    # Decode\n",
        "    decoder_input = [[w2idx['<start>']]]\n",
        "    sentence = []\n",
        "\n",
        "    for _ in range(20):  # Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ Ù¾Ø§Ø³Ø®\n",
        "        decoder_input_padded = tl.prepro.pad_sequences(decoder_input)\n",
        "        decoder_embeddings = model.decoder_embedding(decoder_input_padded)\n",
        "\n",
        "        # ØªÙ†Ø¸ÛŒÙ… state\n",
        "        model.decoder_lstm.initial_state = encoder_states\n",
        "        decoder_outputs = model.decoder_lstm(decoder_embeddings)\n",
        "        logits = model.decoder_dense(decoder_outputs)\n",
        "\n",
        "        # Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù„Ù…Ù‡\n",
        "        probs = tf.nn.softmax(logits[0, -1]).numpy()\n",
        "        top_indices = np.argsort(probs)[-top_n:]\n",
        "        word_id = np.random.choice(top_indices)\n",
        "\n",
        "        if word_id == w2idx['<end>'] or word_id == w2idx['<pad>']:\n",
        "            break\n",
        "\n",
        "        word = idx2w[word_id]\n",
        "        sentence.append(word)\n",
        "        decoder_input[0].append(word_id)\n",
        "\n",
        "    return ' '.join(sentence) if sentence else \"...\"\n",
        "\n",
        "# ============================================\n",
        "# Ø¨Ø®Ø´ 13: ØªØ³Øª ØªØ¹Ø§Ù…Ù„ÛŒ\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ğŸ¤– Ú†Øªâ€ŒØ¨Ø§Øª Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª!\")\n",
        "print(\"=\"*50)\n",
        "print(\"Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø§Ø² Ø³ÙˆØ§Ù„Ø§Øª:\\n\")\n",
        "\n",
        "# ØªØ³Øª Ú†Ù†Ø¯ Ù†Ù…ÙˆÙ†Ù‡\n",
        "test_queries = [\n",
        "    \"hello\",\n",
        "    \"how are you\",\n",
        "    \"what is your name\",\n",
        "    \"thank you\",\n",
        "    \"good morning\",\n",
        "    \"bye\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    response = inference(query, model, top_n=3)\n",
        "    print(f\"You: {query}\")\n",
        "    print(f\"Bot: {response}\\n\")\n",
        "\n",
        "# Ø­Ù„Ù‚Ù‡ ØªØ¹Ø§Ù…Ù„ÛŒ\n",
        "print(\"\\nğŸ’¬ Ø­Ø§Ù„Ø§ Ø´Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø³ÙˆØ§Ù„ Ø¨Ù¾Ø±Ø³ÛŒØ¯ (Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ 'quit' Ø¨Ø²Ù†ÛŒØ¯):\\n\")\n",
        "while True:\n",
        "    try:\n",
        "        query = input(\"You: \")\n",
        "        if query.lower() in ['quit', 'exit', 'q', '']:\n",
        "            print(\"Ø®Ø¯Ø§Ø­Ø§ÙØ¸! ğŸ‘‹\")\n",
        "            break\n",
        "\n",
        "        if query.strip():\n",
        "            response = inference(query, model, top_n=3)\n",
        "            print(f\"Bot: {response}\\n\")\n",
        "    except:\n",
        "        break\n",
        "\n",
        "print(\"\\nâœ… Ø§Ø¬Ø±Ø§ ØªÙ…Ø§Ù… Ø´Ø¯!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcTVxsNkc0yU"
      },
      "source": [
        "Cornell Movie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "arWo35HtktDI",
        "outputId": "69319c21-2db6-4291-dbd6-e7408b27f100"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¥ Downloading Cornell Movie Dialogs Corpus...\n",
            "âœ… Raw data extracted to 'raw_data/'\n",
            "âš™ï¸  Processing data...\n",
            "âœ… Preprocessing complete! Dataset size: 91231\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------------\n",
        "# ğŸ“¥ Cornell Movie Dialogs â€” Preprocessing for Colab\n",
        "# Saves: idx_q.npy, idx_a.npy, metadata.pkl in current directory\n",
        "# ----------------------------------\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pickle\n",
        "import nltk\n",
        "from collections import defaultdict\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Configuration\n",
        "EN_WHITELIST = '0123456789abcdefghijklmnopqrstuvwxyz '\n",
        "UNK = 'unk'\n",
        "VOCAB_SIZE = 8000\n",
        "limit = {'minq': 2, 'maxq': 25, 'mina': 2, 'maxa': 25}\n",
        "\n",
        "# 1. Download Cornell Movie Dialogs Corpus\n",
        "print(\"ğŸ“¥ Downloading Cornell Movie Dialogs Corpus...\")\n",
        "url = \"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\"\n",
        "zip_path = \"cornell_movie_dialogs_corpus.zip\"\n",
        "urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"cornell_raw\")\n",
        "\n",
        "# Move data to expected path\n",
        "raw_data_dir = \"raw_data\"\n",
        "if os.path.exists(raw_data_dir):\n",
        "    shutil.rmtree(raw_data_dir)\n",
        "shutil.move(\"cornell_raw/cornell movie-dialogs corpus\", raw_data_dir)\n",
        "shutil.rmtree(\"cornell_raw\")\n",
        "os.remove(zip_path)\n",
        "\n",
        "print(\"âœ… Raw data extracted to 'raw_data/'\")\n",
        "\n",
        "# 2. Helper functions (from your data.py, adapted)\n",
        "def get_id2line():\n",
        "    lines = open('raw_data/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "    id2line = {}\n",
        "    for line in lines:\n",
        "        _line = line.split(' +++$+++ ')\n",
        "        if len(_line) == 5:\n",
        "            id2line[_line[0]] = _line[4]\n",
        "    return id2line\n",
        "\n",
        "def get_conversations():\n",
        "    conv_lines = open('raw_data/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "    convs = []\n",
        "    for line in conv_lines[:-1]:\n",
        "        _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
        "        convs.append(_line.split(','))\n",
        "    return convs\n",
        "\n",
        "def gather_dataset(convs, id2line):\n",
        "    questions, answers = [], []\n",
        "    for conv in convs:\n",
        "        if len(conv) % 2 != 0:\n",
        "            conv = conv[:-1]\n",
        "        for i in range(len(conv)):\n",
        "            if i % 2 == 0:\n",
        "                questions.append(id2line[conv[i]])\n",
        "            else:\n",
        "                answers.append(id2line[conv[i]])\n",
        "    return questions, answers\n",
        "\n",
        "def filter_line(line, whitelist):\n",
        "    return ''.join(ch for ch in line.lower() if ch in whitelist)\n",
        "\n",
        "def filter_data(qseq, aseq):\n",
        "    filtered_q, filtered_a = [], []\n",
        "    assert len(qseq) == len(aseq)\n",
        "    for i in range(len(qseq)):\n",
        "        qlen = len(qseq[i].split())\n",
        "        alen = len(aseq[i].split())\n",
        "        if limit['minq'] <= qlen <= limit['maxq'] and limit['mina'] <= alen <= limit['maxa']:\n",
        "            filtered_q.append(qseq[i])\n",
        "            filtered_a.append(aseq[i])\n",
        "    return filtered_q, filtered_a\n",
        "\n",
        "def index_(tokenized_sentences, vocab_size):\n",
        "    freq_dist = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "    vocab = freq_dist.most_common(vocab_size)\n",
        "    index2word = ['_'] + [UNK] + [x[0] for x in vocab]\n",
        "    word2index = {w: i for i, w in enumerate(index2word)}\n",
        "    return index2word, word2index, freq_dist\n",
        "\n",
        "def filter_unk(qtokenized, atokenized, w2idx):\n",
        "    filtered_q, filtered_a = [], []\n",
        "    for qline, aline in zip(qtokenized, atokenized):\n",
        "        unk_count_a = sum(1 for w in aline if w not in w2idx)\n",
        "        unk_count_q = sum(1 for w in qline if w not in w2idx)\n",
        "        if unk_count_a <= 2 and (unk_count_q == 0 or unk_count_q / len(qline) <= 0.2):\n",
        "            filtered_q.append(qline)\n",
        "            filtered_a.append(aline)\n",
        "    return filtered_q, filtered_a\n",
        "\n",
        "def zero_pad(qtokenized, atokenized, w2idx):\n",
        "    data_len = len(qtokenized)\n",
        "    idx_q = np.zeros([data_len, limit['maxq']], dtype=np.int32)\n",
        "    idx_a = np.zeros([data_len, limit['maxa']], dtype=np.int32)\n",
        "    for i in range(data_len):\n",
        "        q_idx = [w2idx.get(w, w2idx[UNK]) for w in qtokenized[i]]\n",
        "        a_idx = [w2idx.get(w, w2idx[UNK]) for w in atokenized[i]]\n",
        "        idx_q[i] = (q_idx + [0] * limit['maxq'])[:limit['maxq']]\n",
        "        idx_a[i] = (a_idx + [0] * limit['maxa'])[:limit['maxa']]\n",
        "    return idx_q, idx_a\n",
        "\n",
        "# 3. Main preprocessing pipeline\n",
        "print(\"âš™ï¸  Processing data...\")\n",
        "id2line = get_id2line()\n",
        "convs = get_conversations()\n",
        "questions, answers = gather_dataset(convs, id2line)\n",
        "\n",
        "questions = [filter_line(q, EN_WHITELIST) for q in questions]\n",
        "answers = [filter_line(a, EN_WHITELIST) for a in answers]\n",
        "\n",
        "qlines, alines = filter_data(questions, answers)\n",
        "\n",
        "qtokenized = [[w for w in q.split() if w] for q in qlines]\n",
        "atokenized = [[w for w in a.split() if w] for a in alines]\n",
        "\n",
        "idx2w, w2idx, freq_dist = index_(qtokenized + atokenized, VOCAB_SIZE)\n",
        "qtokenized, atokenized = filter_unk(qtokenized, atokenized, w2idx)\n",
        "\n",
        "idx_q, idx_a = zero_pad(qtokenized, atokenized, w2idx)\n",
        "\n",
        "# Save files\n",
        "np.save('idx_q.npy', idx_q)\n",
        "np.save('idx_a.npy', idx_a)\n",
        "\n",
        "metadata = {\n",
        "    'w2idx': w2idx,\n",
        "    'idx2w': idx2w,\n",
        "    'limit': limit,\n",
        "    'freq_dist': freq_dist\n",
        "}\n",
        "with open('metadata.pkl', 'wb') as f:\n",
        "    pickle.dump(metadata, f)\n",
        "\n",
        "print(f\"âœ… Preprocessing complete! Dataset size: {idx_q.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrEflJzNR_b3"
      },
      "source": [
        "new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKxgk9AD_R7E",
        "outputId": "2ea4daeb-7b22-499d-a89a-a3212e13bdb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/data/twitter\n",
            "--2025-11-11 12:56:25--  https://www.dropbox.com/s/tmfwptbs3q180p0/seq2seq.twitter.tar.gz?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6030:18::a27d:5012\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/krypoulsoiqpvjsi5k7as/seq2seq.twitter.tar.gz?rlkey=tbv0l9ehhztzcewa20zwjaut4&dl=1 [following]\n",
            "--2025-11-11 12:56:25--  https://www.dropbox.com/scl/fi/krypoulsoiqpvjsi5k7as/seq2seq.twitter.tar.gz?rlkey=tbv0l9ehhztzcewa20zwjaut4&dl=1\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc51263bd0e884ef6da97afe2ff1.dl.dropboxusercontent.com/cd/0/inline/C08Jpu8sSS0dLZIxg-FKsDgk4gyhUPd2-CU6gOBrbg4wuJdXsvkW3988Al3iueefYlY3XuWC_OefW-MllBiHTSgjouI9JknXld78azrel82ryxHSR5VDa33z4etKhj1Fa-Y/file?dl=1# [following]\n",
            "--2025-11-11 12:56:26--  https://uc51263bd0e884ef6da97afe2ff1.dl.dropboxusercontent.com/cd/0/inline/C08Jpu8sSS0dLZIxg-FKsDgk4gyhUPd2-CU6gOBrbg4wuJdXsvkW3988Al3iueefYlY3XuWC_OefW-MllBiHTSgjouI9JknXld78azrel82ryxHSR5VDa33z4etKhj1Fa-Y/file?dl=1\n",
            "Resolving uc51263bd0e884ef6da97afe2ff1.dl.dropboxusercontent.com (uc51263bd0e884ef6da97afe2ff1.dl.dropboxusercontent.com)... 162.125.81.15, 2620:100:6030:15::a27d:500f\n",
            "Connecting to uc51263bd0e884ef6da97afe2ff1.dl.dropboxusercontent.com (uc51263bd0e884ef6da97afe2ff1.dl.dropboxusercontent.com)|162.125.81.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/C0_53cMvIrxHXhp1S2QhQNZrRNIax-p865QVp72oWMHBchK44O4pv-G0e9QoEUMXddmko_l2rgqraU9UEWkjRo4vHWWOoBT_MIzsmMWhdk-anbPV7v1mffxui-BHzB8eIyyiKl2B9D89uHMKd0sGRvrH29WFRKsOab6GE6Y8Km8RqGlpZEgsauX3pbYRNl9uOfl-cK54GR6hSysS5CjmfhRfAYcT0onbBa8ZNzwlZxdDlvKQQF_M95uuJdRAL4d8uIH0Qcsr_hb-TfUlI6-TatgYxOQduQJLMh-3gBUsgZsNlx1lkHAZHDxhi-iohr-cf9kNRQpeyTBQOruh5U3atYvVZp58YNT5k7Faj14LD35GLw/file?dl=1 [following]\n",
            "--2025-11-11 12:56:27--  https://uc51263bd0e884ef6da97afe2ff1.dl.dropboxusercontent.com/cd/0/inline2/C0_53cMvIrxHXhp1S2QhQNZrRNIax-p865QVp72oWMHBchK44O4pv-G0e9QoEUMXddmko_l2rgqraU9UEWkjRo4vHWWOoBT_MIzsmMWhdk-anbPV7v1mffxui-BHzB8eIyyiKl2B9D89uHMKd0sGRvrH29WFRKsOab6GE6Y8Km8RqGlpZEgsauX3pbYRNl9uOfl-cK54GR6hSysS5CjmfhRfAYcT0onbBa8ZNzwlZxdDlvKQQF_M95uuJdRAL4d8uIH0Qcsr_hb-TfUlI6-TatgYxOQduQJLMh-3gBUsgZsNlx1lkHAZHDxhi-iohr-cf9kNRQpeyTBQOruh5U3atYvVZp58YNT5k7Faj14LD35GLw/file?dl=1\n",
            "Reusing existing connection to uc51263bd0e884ef6da97afe2ff1.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6298197 (6.0M) [application/binary]\n",
            "Saving to: â€˜seq2seq.twitter.tar.gzâ€™\n",
            "\n",
            "seq2seq.twitter.tar 100%[===================>]   6.01M  28.0MB/s    in 0.2s    \n",
            "\n",
            "2025-11-11 12:56:27 (28.0 MB/s) - â€˜seq2seq.twitter.tar.gzâ€™ saved [6298197/6298197]\n",
            "\n",
            "Sample lines from chat.txt:\n",
            "head: cannot open 'chat.txt' for reading: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.makedirs('data/twitter', exist_ok=True)\n",
        "%cd data/twitter\n",
        "\n",
        "# Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¯Ø§Ø¯Ù‡ Ø¨Ø§ Ù„ÛŒÙ†Ú© Ù…Ø³ØªÙ‚ÛŒÙ… Dropbox\n",
        "!wget -O seq2seq.twitter.tar.gz 'https://www.dropbox.com/s/tmfwptbs3q180p0/seq2seq.twitter.tar.gz?dl=1'\n",
        "\n",
        "# Ø§Ø³ØªØ®Ø±Ø§Ø¬\n",
        "!tar -xzf seq2seq.twitter.tar.gz\n",
        "\n",
        "# Ø¨Ø±Ø±Ø³ÛŒ\n",
        "print(\"Sample lines from chat.txt:\")\n",
        "!head -n 6 chat.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC3RCdSbk9-X"
      },
      "source": [
        "twiter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvTmDp8bqDZv",
        "outputId": "625ff770-0347-4fec-92f6-a7efc0e1eb78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ…   'chat.txt'    made !\n"
          ]
        }
      ],
      "source": [
        "# Ø³Ø§Ø®Øª ÛŒÚ© ÙØ§ÛŒÙ„ Ù†Ù…ÙˆÙ†Ù‡ chat.txt\n",
        "sample_lines = [\n",
        "    \"hi\", \"hello\",\n",
        "    \"how are you\", \"i am fine thanks\",\n",
        "    \"what is your name\", \"i am a chatbot\",\n",
        "    \"where are you from\", \"i live in the cloud\",\n",
        "    \"do you like music\", \"yes i love music\",\n",
        "    \"what can you do\", \"i can chat with you\",\n",
        "    \"tell me a joke\", \"why did the robot go to school to get a little byte\",\n",
        "    \"are you smart\", \"i try my best\",\n",
        "    \"goodbye\", \"see you soon\",\n",
        "    \"thank you\", \"you are welcome\"\n",
        "]\n",
        "\n",
        "with open('chat.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in sample_lines:\n",
        "        f.write(line + '\\n')\n",
        "\n",
        "print(\"âœ…   'chat.txt'    made !\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEjA4Mniq5X3"
      },
      "outputs": [],
      "source": [
        "# --- Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² ---\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)  # Ø¨Ø±Ø§ÛŒ FreqDist Ùˆ tokenization\n",
        "\n",
        "# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª ---\n",
        "EN_WHITELIST = '0123456789abcdefghijklmnopqrstuvwxyz '  # Ø´Ø§Ù…Ù„ ÙØ§ØµÙ„Ù‡\n",
        "EN_BLACKLIST = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "\n",
        "# Ø¯Ø± Ú©ÙˆÙ„Ø¨ØŒ ÙØ§ÛŒÙ„ Ø±Ø§ Ø¯Ø± Ù…Ø³ÛŒØ± Ø±ÛŒØ´Ù‡ Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… (ÛŒØ§ Ø¢Ù¾Ù„ÙˆØ¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…)\n",
        "FILENAME = 'chat.txt'  # ÙØ§ÛŒÙ„ Ø¨Ø§ÛŒØ¯ Ø¯Ø± Ø±ÛŒØ´Ù‡ Ø¨Ø§Ø´Ø¯\n",
        "\n",
        "limit = {\n",
        "    'maxq': 20,\n",
        "    'minq': 1,   # ØªØºÛŒÛŒØ±: Ø§Ø² 0 Ø¨Ù‡ 1 Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø³ÙˆØ§Ù„ Ø®Ø§Ù„ÛŒ\n",
        "    'maxa': 20,\n",
        "    'mina': 3\n",
        "}\n",
        "\n",
        "UNK = 'unk'\n",
        "VOCAB_SIZE = 6000\n",
        "\n",
        "# --- ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ ---\n",
        "import random\n",
        "import sys\n",
        "import nltk\n",
        "import itertools\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# --- ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ ---\n",
        "def read_lines(filename):\n",
        "    with open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        lines = f.read().split('\\n')\n",
        "    return [line.strip() for line in lines if line.strip()]  # Ø­Ø°Ù Ø®Ø·ÙˆØ· Ø®Ø§Ù„ÛŒ\n",
        "\n",
        "def filter_line(line, whitelist):\n",
        "    return ''.join([ch for ch in line if ch in whitelist])\n",
        "\n",
        "def index_(tokenized_sentences, vocab_size):\n",
        "    freq_dist = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "    vocab = freq_dist.most_common(vocab_size)\n",
        "    index2word = ['_'] + [UNK] + [x[0] for x in vocab]\n",
        "    word2index = {w: i for i, w in enumerate(index2word)}\n",
        "    return index2word, word2index, freq_dist\n",
        "\n",
        "def filter_data(sequences):\n",
        "    if len(sequences) % 2 != 0:\n",
        "        sequences = sequences[:-1]  # Ø­Ø°Ù Ø®Ø· Ø¢Ø®Ø± Ø§Ú¯Ø± ÙØ±Ø¯ Ø¨ÙˆØ¯\n",
        "    filtered_q, filtered_a = [], []\n",
        "    raw_data_len = len(sequences) // 2\n",
        "\n",
        "    for i in range(0, len(sequences), 2):\n",
        "        q = sequences[i].strip()\n",
        "        a = sequences[i+1].strip()\n",
        "        if not q or not a:\n",
        "            continue\n",
        "        qlen = len(q.split())\n",
        "        alen = len(a.split())\n",
        "        if limit['minq'] <= qlen <= limit['maxq'] and limit['mina'] <= alen <= limit['maxa']:\n",
        "            filtered_q.append(q)\n",
        "            filtered_a.append(a)\n",
        "\n",
        "    filt_data_len = len(filtered_q)\n",
        "    filtered_percent = int((raw_data_len - filt_data_len) * 100 / raw_data_len) if raw_data_len > 0 else 0\n",
        "    print(f\"{filtered_percent}% filtered from original data\")\n",
        "    return filtered_q, filtered_a\n",
        "\n",
        "def pad_seq(seq, lookup, maxlen):\n",
        "    indices = []\n",
        "    for word in seq:\n",
        "        indices.append(lookup.get(word, lookup[UNK]))\n",
        "    indices += [0] * (maxlen - len(indices))\n",
        "    return indices[:maxlen]  # Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø·ÙˆÙ„ Ø¯Ù‚ÛŒÙ‚\n",
        "\n",
        "def zero_pad(qtokenized, atokenized, w2idx):\n",
        "    data_len = len(qtokenized)\n",
        "    idx_q = np.zeros([data_len, limit['maxq']], dtype=np.int32)\n",
        "    idx_a = np.zeros([data_len, limit['maxa']], dtype=np.int32)\n",
        "\n",
        "    for i in range(data_len):\n",
        "        q_indices = pad_seq(qtokenized[i], w2idx, limit['maxq'])\n",
        "        a_indices = pad_seq(atokenized[i], w2idx, limit['maxa'])\n",
        "        idx_q[i] = q_indices\n",
        "        idx_a[i] = a_indices\n",
        "    return idx_q, idx_a\n",
        "\n",
        "def process_data():\n",
        "    print('\\n>> Reading lines from file...')\n",
        "    if not os.path.exists(FILENAME):\n",
        "        print(f\"âŒ Error: '{FILENAME}' not found!\")\n",
        "        print(\"Please upload 'chat.txt' to Colab (use the file uploader).\")\n",
        "        return\n",
        "\n",
        "    lines = read_lines(FILENAME)\n",
        "    print(f\"âœ… Read {len(lines)} lines.\")\n",
        "\n",
        "    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú©\n",
        "    lines = [line.lower() for line in lines]\n",
        "\n",
        "    # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡\n",
        "    print('\\n:: Sample lines (before filtering):')\n",
        "    for i in range(min(4, len(lines))):\n",
        "        print(f\"  {i}: {repr(lines[i])}\")\n",
        "\n",
        "    # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§\n",
        "    print('\\n>> Filtering characters...')\n",
        "    lines = [filter_line(line, EN_WHITELIST) for line in lines]\n",
        "    lines = [' '.join(line.split()) for line in lines if line.strip()]  # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÙ‡\n",
        "\n",
        "    print('\\n:: Sample lines (after filtering):')\n",
        "    for i in range(min(4, len(lines))):\n",
        "        print(f\"  {i}: {repr(lines[i])}\")\n",
        "\n",
        "    # ÙÛŒÙ„ØªØ± Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø·ÙˆÙ„\n",
        "    print('\\n>> Filtering by length...')\n",
        "    qlines, alines = filter_data(lines)\n",
        "    print(f\"âœ… Kept {len(qlines)} Q/A pairs.\")\n",
        "\n",
        "    if len(qlines) == 0:\n",
        "        print(\"âŒ No valid Q/A pairs found! Check your chat.txt format.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nExample pair:\\n  Q: {qlines[0]}\\n  A: {alines[0]}\")\n",
        "\n",
        "    # ØªÙˆÚ©Ù†Ø§ÛŒØ²\n",
        "    qtokenized = [q.split() for q in qlines]\n",
        "    atokenized = [a.split() for a in alines]\n",
        "\n",
        "    # Ø³Ø§Ø®Øª ÙˆØ§Ú˜Ú¯Ø§Ù†\n",
        "    print('\\n>> Building vocabulary...')\n",
        "    idx2w, w2idx, freq_dist = index_(qtokenized + atokenized, VOCAB_SIZE)\n",
        "    print(f\"âœ… Vocabulary size: {len(idx2w)} (max: {VOCAB_SIZE + 2})\")\n",
        "\n",
        "    # Ù¾Ø¯ Ú©Ø±Ø¯Ù†\n",
        "    print('\\n>> Zero-padding sequences...')\n",
        "    idx_q, idx_a = zero_pad(qtokenized, atokenized, w2idx)\n",
        "\n",
        "    # Ø°Ø®ÛŒØ±Ù‡\n",
        "    print('\\n>> Saving processed data...')\n",
        "    np.save('idx_q.npy', idx_q)\n",
        "    np.save('idx_a.npy', idx_a)\n",
        "\n",
        "    metadata = {\n",
        "        'w2idx': w2idx,\n",
        "        'idx2w': idx2w,\n",
        "        'limit': limit,\n",
        "        'freq_dist': freq_dist\n",
        "    }\n",
        "    with open('metadata.pkl', 'wb') as f:\n",
        "        pickle.dump(metadata, f)\n",
        "\n",
        "    print(\"âœ… Done! Files saved: idx_q.npy, idx_a.npy, metadata.pkl\")\n",
        "\n",
        "def load_data(PATH=''):\n",
        "    metadata = np.load(PATH + 'idx_q.npy'), np.load(PATH + 'idx_a.npy')\n",
        "    with open(PATH + 'metadata.pkl', 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    return meta, metadata[0], metadata[1]\n",
        "\n",
        "# --- Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ ---\n",
        "if __name__ == '__main__':\n",
        "    process_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBojBxpk_7_y"
      },
      "source": [
        "main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05mG_DlVqD5O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPaarOVZSGpK"
      },
      "source": [
        "data"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyM+rEBiQEMORpgENOqcAy6B",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}